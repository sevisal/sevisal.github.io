---
title: "Multi-view hierarchical Variational AutoEncoders with Factor Analysis latent space"
date: 2023-07-01
description: "Under review"
tags: ["Paper", "Bayesian", "Neural networks"]
link: https://arxiv.org/pdf/2207.09185.pdf
cite: https://scholar.googleusercontent.com/scholar.bib?q=info:4wjpQdBL4wcJ:scholar.google.com/&output=citation&scisdr=ClETHKYVEKGE-K5tllg:AFWwaeYAAAAAZTprjljN-eBG_Bsw3E1V0s5oMBE&scisig=AFWwaeYAAAAAZTprjnsOzk-XGbneXeFcVeOIlRk&scisf=4&ct=citation&cd=-1&hl=es&scfhb=1
state: Under review
type: post
weight: 25
showTableOfContents: true
---

Real-world databases are complex and usually require dealing with heterogeneous and mixed data types making the exploitation of shared information between views a critical issue. For this purpose, recent studies based on deep generative models merge all views into a nonlinear complex latent space, which can share information among views. However, this solution limits the modelâ€™s interpretability, flexibility, and modularity. We propose a novel method to
overcome these limitations by combining multiple Variational AutoEncoders (VAE) with a Factor Analysis latent space (FA-VAE). We use VAEs to learn a
private representation of each heterogeneous view in a continuous latent space. Then, we share the information between views by a low-dimensional latent space
using a linear projection matrix. This way, we create a flexible and modular hierarchical dependency between private and shared information in which new
views can be incorporated afterwards. Beyond that, we can condition pre-trained models, cross-generate data from different domains, and perform transfer learning
between generative models.